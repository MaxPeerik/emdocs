# Runtime release notes (emc 3.0.2)
Major update that switches the underlying messaging technology to use AMQP over Websocket Secure protocol.
## Major changes
- The eMagiz Mendix Connector now connects to the eMagiz bus using the AMQP over WebSocket Secure (amqpwss) protocol. This should be better compatible with most current and future cloud infrastructures. Also, by switching to a standardized protocol, the new eMagiz Mendix Connector provides better forward compatibility with future changes to the eMagiz messaging technology.
- When sending messages asynchronously from Mendix to the eMagiz bus, the eMagiz Mendix Connector buffers these messages internally in case the bus is unreachable. In previous versions these outgoing messages were always stored in an embedded JMS server first, and then forwarded to the eMagiz bus. From this version onwards the eMagiz Mendix Connector no longer contains an embedded JMS server, but uses the Mendix database to temporarily store these messages. Functionally speaking the overall messaging behaviour is the same, but there are some notable differences:
  - Messages are now only stored locally when the connection to the eMagiz bus is down. During normal operations messages are send in real-time, which might (slightly) increase performance.
  - Max limit for locally stored runtime metrics was 5 MiB and is now 1000 messages. In practice this should be quite similar. The maximum for "normal" user-defined messages is still unlimited.
  - A button "queued messages" has been added that shows how many messages are currently stored locally, waiting to be send to the bus. Previously there was no way to to see this.
  - Mendix logging has been added to inform about messages being stored locally. During startup, the number of messages that are still present in the local message store is logged. The message "switching to 'offline' mode: messages are now being stored locally" is logged when the first message cannot be delivered in real-time to the eMagiz bus, and the message "switching back to 'online' mode: all locally stored messages have been send" is logged when the real-time delivery is available again and the full backlog of stored messages has been processed.
  - To reduce the amount of disk space used, all messages are now compressed before storing them locally.
- Changed and updated many dependencies in 'userlib'. Make sure to run the cleanup tool ('resources/emagiz-cleanup-tool.jar') to remove all old dependencies.
  - Replaced HornetQ 2.3.25 with Qpid JMS 0.40.0 and Proton-J 0.31.0
  - Updated Spring Framework from 4.3.8 to 4.3.20
  - Updated Spring Integration from 4.3.9 to 4.3.17
  - Updated Spring Retry from 1.1.5 to 1.2.2
  - Updated Netty from 3.6.10 to 4.1.34
  - Updated JMS spec from 1.1 to 2.0
  - Updated Joda Time from 2.7.0 to 2.10.1
  - Updated Commons Codec from 1.9.0 to 1.11.0
  - Added JTA spec 1.1
  - Removed JBoss Logging 3.1.4
- From this release onwards, Mendix 5 is no longer supported.
## Known issues
- In older versions of the Mendix Desktop Modeler, "emulate cloud security" may cause the eMagiz Mendix connector to fail to connect. This will not occur in the actual cloud. From Mendix 7.21.0 on, this setting has been removed.
## Remarks
- This version is NOT compatible with HornetQ. Be sure to first upgrade your JMS server to accept AMQP connections before using this connector version to connect to it.
- Versions 3.0.0 and 3.0.1 were given out to a select number of early adopter buses, but were never GA releases. These release notes therefore list all cumulative changes since the last public release (version 2.5.0). We strongly recommend upgrading from versions 3.0.0 and 3.0.1, since 3.0.2 contains an important fix: queue prefetch was enabled to prevent large messages getting stuck in queues.
- Version 3.x of the connector is fully compatible with flows build for version 2.x with one exception: the connector infra needs to be updated to switch from HornetQ to AMQP connections. You can simply tell the difference by the build number of the packages: build number 2 is for EMC version 2.x (HornetQ) and build number 3 is for EMC version 3.x (AMQP).

# Portal release notes (8.0.0)
Release that switches the underlying messaging technology of eMagiz buses.
Furthermore, several components were added to the flow designer that enable you to connect to Kafka. In addition, this release includes several other improvements and tackles some minor issues.
## New features
- Added new components 'Kafka message driven channel adapter' and 'Kafka outbound channel adapter' to the flow designer. Together with new support objects 'Kafka message listener container' and 'Kafka template', these components allow you to receive message from and send messages to the Apache Kafka distributed streaming platform.
- Added new support object 'WS-Addressing action callback' to the flow designer. This allows you to add WS-Addressing headers such as 'wsa:Action' and 'wsa:To' to the SOAP requests of a web service outbound gateway.
## Major changes
- eMagiz now no longer use HornetQ as the (open source) JMS server, but we are switching to Apache ActiveMQ Artemis. The main reason for this is not that we were unhappy with HornetQ, but simply because HornetQ development has stopped. However, all source code has been donated to the Apache ActiveMQ community, and they released Artemis as the direct successor of HornetQ. Other than multiple years of improvements, bugfixes and performance gains, the most notable changes for eMagiz users are:
  - Auto-creation of queues: Artemis will automatically create queues when the first consumer is registered on a queue or when the first message is send to a queue. This means that the JMS server flow no longer contains a list of all queue names and does not have to be redeployed every time a process is added to your bus, resulting in no downtime of existing integrations during deployments.
  - Auto-deletion of queues: Artemis will automatically delete queues when the last consumer is disconnected from a queue and no messages are in the queue. This means that the JMS server does not have to be redeployed every time a process is removed from your bus, resulting in no downtime of existing integrations during deployments.
  - Backwards compatible with HornetQ: older flows that still use HornetQ clients can connect to Artemis servers, but new AMQP-based flows cannot connect to HornetQ servers. This means that it is possible to migrate existing buses one runtime at a time, as long as the JMS server is migrated first.
- Related to replacing the HornetQ server with Artemis, we are also replacing the HornetQ client with the Apache Qpid JMS client. Previously HornetQ was used both as the JMS server and client, but because we are switching to the AMQP protocol (see below) we no longer have the requirement of running the same software on both the client and server side. Other than Qpid being actively developed and up-to-date with security patches, the most notable changes for eMagiz users are:
  - JMS connection factory sharing: Qpid connection settings are only stored in each container's infra flow and shared with all other flows in the same runtime, while previously the HornetQ connection settings were stored in each flow separately. This means that when there is a change in these settings, only the infra flows are affected and need to be redeployed instead of every single flow in your bus.
  - Connections with Qpid are secured with two-way TLS using auto-generated certificates, similar to how HornetQ connections are secured. To migrate existing HornetQ-based buses, it is important that those buses no longer use (old) custom SSL settings. Also with Qpid, having to add your bus certificates manually to key-/truststores when altering the Java defaults should no longer be needed.
  - Fully compatible with existing flows: Qpid JMS communicates with the components in your flows using the same JMS API that the HornetQ client uses. This means that after migrating flows to AMQP, the only changed components are some support objects, but all functional flow components – including the JMS inbound and outbound channel adapters – will not be touched at all.
- eMagiz now uses the AMQP protocol (Advanced Message Queuing Protocol) to connect all flows in a message bus, while previously a custom TCP protocol was used. As a user, you will probably never notice the actual protocol being used, but switching to AMQP does bring some important benefits:
  - To send and receive messages, eMagiz now uses the AMQP over WebSocket Secure (amqpwss) protocol. Because this an IETF standardized protocol that is HTTP(S) based, it should be better compatible with most current and future cloud infrastructures compared to the custom TCP protocol we used previously.
  - By switching to a standardized protocol, flows using AMQP provide better forward compatibility with future changes to the eMagiz messaging technology. For example, it should now be possible to completely replace the JMS server with another AMQP compatible message broker without it affecting any of your flows (well, in theory at least).
  - Because the implementations of the AMQP protocol that eMagiz uses are different for the client side (Qpid) and server side (Artemis), this makes updating either of them much easier. Previously, an upgrade of HornetQ would affect every single flow in a bus, but now an upgrade of Artemis will only affect the JMS server flow(s) for example.
- To locally buffer outgoing messages in a runtime, in case of (AMQP) connection problems, we now use a 'channel message store' backed by an embedded H2 database. While functionally very similar to how it worked before – buffering messages in an embedded HornetQ server and then bridging them to the main JMS server – there are some notable differences:
  - Instead of being hidden, this buffering is now transparently shown in the flow designer as a message channel with a special 'storage' icon on it. New support objects that represent the actual message store, an embedded H2 database in this case, are also included in these flows. Note that these support objects are actually located in the infra flow and then shared with other flows that use them.
  - Max limit for locally stored runtime metrics and log entries was 2 x 5 MiB and is now 2 x 1000 messages. In practice this should be quite similar. The maximum for "normal" user-defined messages is still unlimited.
  - The H2 database is writing all data to disk using encrypted (AES-128) files with randomly generated and unique encryption keys. This means that even when deployed on servers without disk encryption, messages are never simply stored in a plain format.
  - The maximum rate when forwarding messages from the local buffer to the JMS server is two messages per second (per message queue), while previously this was unlimited. This lowering of the maximum throughput is intentional, to alleviate the total load on the JMS server when recovering from (long) periods of unavailability. The disadvantage of course is that it can take a bit longer for the backlog of messages to be processed.
  - Since this embedded H2 database is a fully fledged SQL database, it can be used for other storage tasks as well. Right now eMagiz can also use it as the job repository for data pipelines, and expect more applications of this storage mechanism in the future.
## Minor changes
- Updated library 'jsoup' from 1.11.2 to 1.11.3 (prevents XSS attacks from user-submitted content in the portal)
- Removed the documentation menu option from the Create phase of the ILM, because it was not being used while taking up (precious) space in the navigation menu.
- The eMagiz AWS Cloud now has fixed IP addresses (52.57.7.213 and 35.158.233.116) for traffic going to the cloud. This can be useful for IP whitelisting, although hostname based whitelisting is preferred. Note that these IP addresses cannot be used to connect to; for that the 'cloudXXXX.emagizcloud.com' DNS names must be used. Also note that existing AWS Cloud slots are switched to these IP addresses the first time the "update AWS" button is used.
- Across the portal several (help) texts have been improved. For example, providing an extra reminder to turn monitoring of cloud machines back on. (#427155, 428594)
- Deploy - Releases: When properties are missing for flows that should be installed, you can now see exactly which ones are missing instead of being shown a list of all properties of that runtime. In addition, you can fill in their values on the spot.
## Bug fixes
- Design - Cloud Architecture: Removed non-functional "Export to PNG" button.
- Deploy - Releases:
  - Adding new tenants to multi-instance multi-tenant setups now creates the correct containers, and adds the flows that should run to them automatically.
  - After migrating to releases, the request handler and infra flows are added to the containers for Acceptance and Production now. This happens when approving the release for those environments.
  - (Re-)Adding integrations with an entry flow to an eMagiz-Mendix Connector system, now also adds the request handler to the release.
  - Fixed an issue that, in a specific case, would block you from using "Update all flows to the latest versions".
  - Fixed an issue where, in specific cases, the colours would not update correctly while adding or removing integrations to a release.
  - Package overview shows packages in view-only mode again.
## Remarks
- Using both HornetQ (old) and Artemis/Qpid (new) components in the same flow is not supported and will most likely cause unexpected behaviour.
- Flows that use AMQP require Java 8. While technically HornetQ-based flows can still run on Java 7, we strongly recommend to update all existing runtimes to Java 8, since Java 7 has been end-of-life for four years now.
- As an alternative to the Oracle Java 8 download, consider trying AdoptOpenJDK (see https://adoptopenjdk.net), which has better long-term support. While we do not have much experience with this distribution (yet), initial tests show no problems running eMagiz on it.
- A select number of early adopter buses are already running on AMQP. We strongly recommend these buses to 'reset' and redeploy all their infra flows and JMS server(s), because this release contains an important fix: queue prefetch was enabled to prevent large messages getting stuck in queues.
